# KNN Distance Metric Comparison for Exoplanet Detection  
**Dataset:** NASA Exoplanet Archive â€“ Kepler Candidates  
**Task:** Binary Classification (Confirmed Planet vs False Positive)  
**Models Implemented:** KNN from scratch with multiple distance metrics

---

## ğŸ“Œ Project Overview

This project implements the **K-Nearest Neighbors (KNN)** and **Logistic Regression** algorithm **from scratch in Python** and evaluates how different **distance metrics** affect classification accuracy on a real scientific dataset:  
NASAâ€™s Kepler exoplanet candidate catalog.

The goal is to understand how different classification model behave for the same dataset we also used different geometric interpretations of distance to improve exoplanet detection performance.

---

## ğŸ“Š Dataset Description

We use the **Kepler â€œcumulativeâ€ candidate catalog** from the NASA Exoplanet Archive.
###Link: https://www.kaggle.com/datasets/nasa/kepler-exoplanet-search-results
### Features used:
- `koi_period` â€“ Orbital period  
- `koi_depth` â€“ Transit depth  
- `koi_duration` â€“ Transit duration  
- `koi_prad` â€“ Planet radius (Earth radii)  
- `koi_teq` â€“ Equilibrium temperature  
- `koi_insol` â€“ Insolation flux  
- `koi_steff` â€“ Stellar temperature  
- `koi_srad` â€“ Stellar radius  
- `koi_smass` â€“ Stellar mass  

### Target label:
- **1** â†’ Confirmed Planet  
- **0** â†’ False Positive  

Only rows with valid Kepler dispositions were included.

---

## ğŸ§  Distance Metrics Implemented

### 1ï¸âƒ£ **Euclidean Distance**
\[
d = \sqrt{\sum (x_i - y_i)^2}
\]

### 2ï¸âƒ£ **Manhattan Distance**
\[
d = \sum |x_i - y_i|
\]

### 3ï¸âƒ£ **Mahalanobis Distance**
\[
d = \sqrt{(x-y)^T S^{-1}(x-y)}
\]
Accounts for **feature correlations** using the covariance matrix.

### 4ï¸âƒ£ **RBF Kernel Distance**
\[
sim = e^{-\gamma \|x-y\|^2}, \quad d = 1 - sim
\]
A nonlinear distance metric.

### 5ï¸âƒ£ **RBF Normalized**
Scaled variant:

\[
d = \frac{\|x-y\|^2}{n_{\text{features}}}
\]

---

## ğŸ§ª Experimental Results

| Distance Metric | Accuracy | Notes |
|------------------|----------|------------------------------|
| **Euclidean** | **78.5%** | Baseline distance metric |
| **Manhattan** | **78.5%** | Same neighbor ordering as Euclidean after scaling |
| **Mahalanobis** | **84.5%** | Best performance; accounts for astrophysical correlations |
| **RBF (Î³=0.01)** | **78.03%** | Behaves similarly to Euclidean (small Î³ â†’ linear) |
| **RBF (Î³=0.1)** | **64.4%** | Î³ too large â†’ nonlinear distortion hurts performance |
| **RBF (Î³=0.0001)** | **78.3%** | Again approximates Euclidean |
| **Logistic Regression (baseline)** | **82.5%** | Shows approximate linear separability |

---

## ğŸ“Œ Scientific Insights

### ğŸ”¹ 1. **Kepler features are moderately linearly separable**
This is why logistic regression performs well (82.5%).

### ğŸ”¹ 2. **Euclidean and Manhattan give identical accuracy**
After feature standardization, both metrics induce nearly identical neighborhood structure.

### ğŸ”¹ 3. **Mahalanobis is the best performer (84.5%)**
Because:
- transit features are **correlated**  
- Mahalanobis "whitens" the feature space  
- improving nearest-neighbor geometry  

This is expected in astrophysical datasets where stellar parameters influence multiple planetary observables.

### ğŸ”¹ 4. **RBF kernel does NOT improve performance**
The dataset does not exhibit strong nonlinear boundaries.

With large Î³, RBF collapses distances and performance drops significantly (~64%).

With small Î³, RBF behaves like Euclidean â†’ 78%.

You can test with different Î³ yourself and see the difference.
---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ knn.py # KNN implementation with all distance metrics
â”œâ”€â”€ preprocess.py # Data loading and cleaning for Kepler dataset
â”œâ”€â”€ Kepler_Identification_KNN.py # Using KNN on the Kepler dataset using different distance fucntions
â”œâ”€â”€ Kepler_identification_Lg.py # Using Logistic regression on the Kepler dataset
â”œâ”€â”€ Knn_result.png # Accuracy bar graph for KNN representing the accuracy of different dataset fucnctions
â”œâ”€â”€ Lg_result.png # Shows the ROC_AOC curver and cost fucntion mapping for the logistic regression model
â”œâ”€â”€ LICENSE # MIT license for the project
â””â”€â”€ README.md
```
---
## ğŸš€ How to Run
If you want to runt he logistic regression model:
```
python Kepler_Identification_Lg.py
```
If you want to run the KNN model:
```
python Kepler_Identification_KNN.py
```
## ğŸ Conclusion

This project demonstrates how distance metrics fundamentally change the geometry of KNN, especially in scientific datasets.

Key takeaway:
Exoplanet detection from Kepler features is not strongly non-linear,but is influenced by correlated astrophysical parameters.
When features are correlated (as in astrophysics), Mahalanobis distance is superior.

## ğŸ“ Contact
Feel free to explore the repo and message me about improvements or questions!
Email:yashasvi21022005@gmail.com
LinkedIn:https://www.linkedin.com/in/yashasvi-kumar-tiwari/


